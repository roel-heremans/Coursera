{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Evaluating an N-Gram Language Model\n",
    "\n",
    "In this lab, you will evaluate the quality of an n-gram language model using perplexity.\n",
    "\n",
    "We have built several n-gram language models and provided an implementation for computing the probabilities. The implementation includes [Laplace Smoothing](https://en.wikipedia.org/wiki/Additive_smoothing), with assigns some probability to sequences that were never encountered during training.\n",
    "\n",
    "First, review the implementation below to make sure that it makes sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "OOV = '<OOV>'\n",
    "class NGramLM:\n",
    "    def __init__(self, path, smoothing=0.01, verbose=False):\n",
    "        with open(path, 'rb') as fin:\n",
    "            data = pickle.load(fin)\n",
    "        \n",
    "        self.n = data['n']\n",
    "        self.V = set(data['V'])\n",
    "        self.model = data['model']\n",
    "        self.smoothing = smoothing\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def get_prob(self, context, token):\n",
    "        # Take only the n-1 most recent context (Markov Assumption)\n",
    "        context = tuple(context[-self.n+1:])\n",
    "        # Add <BOS> tokens if the context is too short, i.e., it's at the start of the sequence\n",
    "        while len(context) < (self.n-1):\n",
    "            context = (BOS,) + context\n",
    "        # Handle words that were not encountered during the training by replacing them with a special <OOV> token\n",
    "        context = tuple((c if c in self.V else OOV) for c in context)\n",
    "        if token not in self.V:\n",
    "            token = OOV\n",
    "        if context in self.model:\n",
    "            # Compute the probability using a Maximum Likelihood Estimation and Laplace Smoothing\n",
    "            count = self.model[context].get(token, 0)\n",
    "            prob = (count + self.smoothing) / (sum(self.model[context].values()) + self.smoothing * len(self.V))\n",
    "        else:\n",
    "            # Simplified formula if we never encountered this context; the probability of all tokens is uniform\n",
    "            prob = 1 / len(self.V)\n",
    "        # Optional logging\n",
    "        if self.verbose:\n",
    "            print(f'{prob:.4n}', *context, '->', token)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-built n-gram languae models\n",
    "model_unigram = NGramLM('arthur-conan-doyle.tok.train.n1.pkl')\n",
    "model_bigram = NGramLM('arthur-conan-doyle.tok.train.n2.pkl')\n",
    "model_trigram = NGramLM('arthur-conan-doyle.tok.train.n3.pkl')\n",
    "model_4gram = NGramLM('arthur-conan-doyle.tok.train.n4.pkl')\n",
    "model_5gram = NGramLM('arthur-conan-doyle.tok.train.n5.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to see how well these models fit our data! We'll use Perplexity for this calculation, but it's up to you to implement it below.\n",
    "\n",
    "Recall the formula for perplexity from the lecture:\n",
    "\n",
    "$$\n",
    "perplexity = 2^{\\frac{-1}{n}\\sum \\log_2(P(w_i|w_{<i}))}\n",
    "$$\n",
    "\n",
    "Hint: you'll want to use the [`math.log2`](https://docs.python.org/3/library/math.html#math.log2) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10914.060522177839\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "import math\n",
    "def perplexity(model: NGramLM, texts: List[Tuple[str]]) -> float:\n",
    "    pass\n",
    "    # your code here\n",
    "    \n",
    "    # Initialize variables\n",
    "    total_log_prob = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        #print(\"text: {}\".format(text))\n",
    "        #print('Use all tokens up to the current token as context')\n",
    "        for i in range(len(text)):\n",
    "            context = text[:i]  # Use all tokens up to the current token as context\n",
    "            token = text[i]\n",
    "            #print(\"context: {}\".format(context))\n",
    "            #print(\"token: {}\".format(token))\n",
    "            #print(\"Prob(token|context): {}\".format(model.get_prob(context, token)))\n",
    "\n",
    "            # Calculate log probability using the language model\n",
    "            log_prob = -math.log2(model.get_prob(context, token))\n",
    "            #print(\"log_prob: {}\".format(log_prob))\n",
    "\n",
    "            # Accumulate log probabilities\n",
    "            total_log_prob += log_prob\n",
    "            total_tokens += 1\n",
    "\n",
    "    # Calculate perplexity\n",
    "    perplexity_value = 2 ** (total_log_prob / total_tokens)\n",
    "    return perplexity_value\n",
    "    \n",
    "\n",
    "# Example:\n",
    "print(perplexity(model_unigram, [('My', 'dear', 'Watson', '.'), ('Come', 'over', 'here', '!')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "\n",
    "assert round(perplexity(model_unigram, [('My', 'dear', 'Watson')])) == 7531\n",
    "assert round(perplexity(model_bigram, [('My', 'dear', 'Watson')])) == 24\n",
    "assert round(perplexity(model_trigram, [('My', 'dear', 'Watson')])) == 521"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's see how well the model fits a held-out test set.\n",
    "\n",
    "The test data covers a few of the stories, and represents about 12% of the total data.\n",
    "\n",
    "Your task it to print the perplexity for the unigram, bigram, trigram, 4-gram, and 5-gram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Gram: 14924.23177561972\n",
      "2Gram: 259.53894955494343\n",
      "3Gram: 1306.553535960716\n",
      "4Gram: 4921.800243745181\n",
      "5Gram: 8463.320537397633\n"
     ]
    }
   ],
   "source": [
    "toks_test = []\n",
    "with open('arthur-conan-doyle.tok.test.txt', 'rt') as fin:\n",
    "    for line in fin:\n",
    "        toks_test.append(list(line.split()))\n",
    "\n",
    "# your code here\n",
    "print(\"1Gram: {}\".format(perplexity(model_unigram, toks_test)))\n",
    "print(\"2Gram: {}\".format(perplexity(model_bigram, toks_test)))\n",
    "print(\"3Gram: {}\".format(perplexity(model_trigram, toks_test)))\n",
    "print(\"4Gram: {}\".format(perplexity(model_4gram, toks_test)))\n",
    "print(\"5Gram: {}\".format(perplexity(model_5gram, toks_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the perplexity for the bigram model is lower than the others. What does this indicate?\n",
    "\n",
    "The lecture mentioned that it's a bad idea to determine the quality of a model based on the perplexity of data that was used for training. Below, evaluate the same five models using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Gram: 14976.720310596447\n",
      "2Gram: 89.24191939089853\n",
      "3Gram: 91.8592986931776\n",
      "4Gram: 121.15467462849904\n",
      "5Gram: 137.07146227562924\n"
     ]
    }
   ],
   "source": [
    "toks_train = []\n",
    "with open('arthur-conan-doyle.tok.train.txt', 'rt') as fin:\n",
    "    for line in fin:\n",
    "        toks_train.append(list(line.split()))\n",
    "\n",
    "# your code here\n",
    "print(\"1Gram: {}\".format(perplexity(model_unigram, toks_train)))\n",
    "print(\"2Gram: {}\".format(perplexity(model_bigram, toks_train)))\n",
    "print(\"3Gram: {}\".format(perplexity(model_trigram, toks_train)))\n",
    "print(\"4Gram: {}\".format(perplexity(model_4gram, toks_train)))\n",
    "print(\"5Gram: {}\".format(perplexity(model_5gram, toks_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that you get much lower perplexities when measuring on the training data, especially for the models with larger values of `n`. This suggests that the model is over-fitting to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Extras:\n",
    " - In the models we explore above, we use smoothing. What happens to perplexity calculations when smoothing isn't applied? You can try this out by setting `smoothing=0`.\n",
    " - Interpolated or \"back-off\" smoothing is sometimes used in n-gram language models. This techniques computes the weighted average probability of models with different values of `n`. Try implementing this yourself. How does it affect the perplexity on the held-out test set? What about the perplexity on the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
